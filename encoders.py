import os
from argparse import Namespace
from collections import Counter
import json
import re
import string

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm_notebook

class NMTEncoder(nn.Module):
    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):
        """
        Args:
            num_embeddings (int): number of embeddings is the size of source vocabulary
            embedding_size (int): size of the embedding vectors
            rnn_hidden_size (int): size of the RNN hidden state vectors 
        """
        super(NMTEncoder, self).__init__()
    
        self.source_embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)
        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)
    
    def forward(self, x_source, x_lengths):
        """The forward pass of the model
        
        Args:
            x_source (torch.Tensor): the input data tensor.
                x_source.shape is (batch, seq_size)
            x_lengths (torch.Tensor): a vector of lengths for each item in the batch
        Returns:
            a tuple: x_unpacked (torch.Tensor), x_birnn_h (torch.Tensor)
                x_unpacked.shape = (batch, seq_size, rnn_hidden_size * 2)
                x_birnn_h.shape = (batch, rnn_hidden_size * 2)
        """
        print("Inputs are equal")
        print(torch.equal(x_source[0], x_source[1]))
        # The sequence goes through the embedding
        x_embedded = self.source_embedding(x_source)
        
        # print("After embedding")
        # print(torch.equal(x_embedded[0], x_embedded[1]))

        # create PackedSequence; x_packed.data.shape=(number_items, embeddign_size)
        # Packing all the sequences within the batch
        x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(), 
                                        batch_first=True)
        
        #h_n of shape (num_layers * num_directions, batch, hidden_size):

        # Generate the output for all the sequences within the batch
        # final hidden state = (NUM OF LAYERS * NUM DIRECTIONS, BATCH, HIDDEN SIZE)

        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)
        x_birnn_out, x_birnn_h  = self.birnn(x_packed)


        # permute to (BATCH, NUM OF LAYERS * NUM DIRECTIONS, HIDDEN SIZE)
        x_birnn_h = x_birnn_h.permute(1, 0, 2)
        
        # flatten features; reshape to (BATCH, NUM OF LAYERS * NUM DIRECTIONS * HIDDEN SIZE)
        # In our case it is (BATCH, 1 * 2 * HIDDEN SIZE) - this is concatenating the last unit

        #  (recall: -1 takes the remaining positions, 
        #           flattening the two RNN hidden vectors into 1)
        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)
        
        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)
        
        # print("The outputs of the last layer")
        # print(torch.equal(x_unpacked[0], x_unpacked[1]))
        
        # print("The last hidden states")
        # print(torch.equal(x_birnn_h[0], x_birnn_h[1]))
        
        return x_unpacked, x_birnn_h